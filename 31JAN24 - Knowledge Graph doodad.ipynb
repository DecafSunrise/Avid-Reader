{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd74280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from geotext import GeoText\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_lg')\n",
    "def NER(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    return {idx:\n",
    "                {\"text\": ent.text,\n",
    "                 \"tag\": ent.label_,\n",
    "                 \"span\": (ent.start_char, ent.end_char)} for idx, ent in enumerate(doc.ents)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67430a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"12JAN24 - One week news dump.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b6b03e",
   "metadata": {},
   "source": [
    "- ## Time\n",
    "    - Extracted from doc\n",
    "    - Publish date\n",
    "- ## Location\n",
    "    - Geonym/Toponym extraction\n",
    "    - Disambiguation\n",
    "    - Mordecai? \n",
    "- ## Actor\n",
    "    - NER\n",
    "    - Disambiguation\n",
    "- ## Topic\n",
    "    - Atomic Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154023e1",
   "metadata": {},
   "source": [
    "------\n",
    "## Geoparsing\n",
    "- This'll take just a few seconds; 4 seconds per 24k docs in testing\n",
    "- This will identify countries and cities in the text, as possible Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a12244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24103/24103 [00:04<00:00, 5516.23it/s]\n"
     ]
    }
   ],
   "source": [
    "df['GeoText'] = df['text'].progress_apply(GeoText)\n",
    "\n",
    "df['cities'] = df['GeoText'].apply(lambda x: x.cities)\n",
    "df['country_mentions'] = df['GeoText'].apply(lambda x: x.country_mentions)\n",
    "df['countries'] = df['GeoText'].apply(lambda x: x.countries)\n",
    "df['nationalities'] = df['GeoText'].apply(lambda x: x.nationalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd80e47",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "- This'll take quite some time; 32 minutes per 24k docs in testing\n",
    "- This'll scare up candidates for Actors, but also potentially locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "062fb703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 24103/24103 [32:13<00:00, 12.47it/s]\n"
     ]
    }
   ],
   "source": [
    "df['ner'] = df['text'].progress_apply(NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e76bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ent = pd.DataFrame()\n",
    "\n",
    "def extend_df_ent(docid, ner_object):\n",
    "    \n",
    "    global df_ent\n",
    "    \n",
    "    df_temp = pd.DataFrame([ner_object[x] for x in ner_object])\n",
    "    df_temp['docid'] = docid\n",
    "    df_ent = df_ent.append(df_temp)\n",
    "    \n",
    "    del df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c65ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the df_ent object\n",
    "df.progress_apply(lambda row: extend_df_ent(row['docid'], row['ner']), axis=1)\n",
    "## Reorder columns to be something more useful\n",
    "df_ent= df_ent[['docid', 'tag', 'span', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01bb8714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DATE', 'ORG', 'GPE', 'PERSON', 'TIME', 'CARDINAL', 'PRODUCT',\n",
       "       'FAC', 'MONEY', 'PERCENT', 'WORK_OF_ART', 'QUANTITY', 'LOC',\n",
       "       'ORDINAL', 'NORP', 'LAW', 'LANGUAGE', 'EVENT'], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ent['tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0f7f6a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PERSON         396368\n",
       "ORG            352781\n",
       "DATE           290134\n",
       "GPE            218641\n",
       "CARDINAL       151793\n",
       "NORP            65876\n",
       "ORDINAL         36477\n",
       "TIME            31383\n",
       "WORK_OF_ART     30767\n",
       "MONEY           28638\n",
       "PRODUCT         22068\n",
       "LOC             21943\n",
       "PERCENT         17223\n",
       "FAC             16359\n",
       "QUANTITY        14975\n",
       "EVENT           13707\n",
       "LAW              4919\n",
       "LANGUAGE         1119\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ent['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6f61e25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trump              12424\n",
       "Biden               7646\n",
       "Haley               4133\n",
       "Epstein             3244\n",
       "Donald Trump        2964\n",
       "Austin              2660\n",
       "Barbie              2575\n",
       "DeSantis            2380\n",
       "Joe Biden           2170\n",
       "Kate                1282\n",
       "Johnson             1263\n",
       "Nikki Haley         1239\n",
       "Taylor Swift        1211\n",
       "Blinken             1145\n",
       "Max                 1091\n",
       "Derek               1043\n",
       "Ron DeSantis         921\n",
       "Instagram            919\n",
       "Margot Robbie        863\n",
       "Smith                794\n",
       "Swift                792\n",
       "Taylor               776\n",
       "Jeffrey Epstein      768\n",
       "Hunter               738\n",
       "Prince Andrew        734\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ent[df_ent['tag']=='PERSON']['text'].value_counts()[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a3d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_swifter",
   "language": "python",
   "name": "nlp_swifter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3ba496-0b38-40ca-9d9a-985a8c8455a3",
   "metadata": {},
   "source": [
    "Borrowing code for knowledge graph generation with LLMs!\n",
    "\n",
    "https://towardsdatascience.com/leverage-keybert-hdbscan-and-zephyr-7b-beta-to-build-a-knowledge-graph-33d7534ee01b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e3697a5-d3e5-4766-9574-94b1f96e00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af96d14-ab6b-4888-85a4-36ba7a4d6ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee091442fb534a5bbee49a72b02b3c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/4.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74b48aaf7254a0a97112e2c1d723f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8977168a45344de2b6d9fa00088cfcc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987ac9ad0f6d4dc1827543bbb9e61f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bdd8541de84ac3b4d4d16a6869aa51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24471304960f49ad8839b20b0c941d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Required imports\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the model and the tokenizer\n",
    "model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"cuda\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\") # change revision for a different branch\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, \n",
    "                     use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7fd26-632a-4e42-859b-e3437949fe4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8853e2cc-76e6-4a11-9318-220aaa73bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "from keybert.llm import TextGeneration\n",
    "from keybert import KeyLLM, KeyBERT\n",
    "\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(r\"12JAN24 - One week news dump.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a8561aa-cb69-4559-b1f8-7dd3bf94bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ee8e316-d917-40ed-a301-3c74626cebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"The Bronx (/brɒŋks/) is a borough of New York City, coextensive with Bronx County, in the U.S. state of New York. It is south of Westchester County; north and east of the New York City borough of Manhattan, across the Harlem River; and north of the New York City borough of Queens, across the East River. The Bronx has a land area of 42 square miles (109 km2) and a population of 1,472,654 in the 2020 census.[1] If each borough were ranked as a city, the Bronx would rank as the ninth-most-populous in the U.S. Of the five boroughs, it has the fourth-largest area, fourth-highest population, and third-highest population density.[4] The population density of the Bronx was 32,718.7 inhabitants per square mile (12,632.8/km2) in 2022, the third-highest population density of any county in the United States, behind Manhattan and Brooklyn.[5] It is the only borough of New York City not primarily on an island. With a population that is 54.8% Hispanic as of 2020, it is the only majority-Hispanic county in the Northeastern United States and the fourth-most-populous nationwide.[6]\n",
    "\n",
    "The Bronx is divided by the Bronx River into a hillier section in the west, and a flatter eastern section. East and west street names are divided by Jerome Avenue. The West Bronx was annexed to New York City in 1874, and the areas east of the Bronx River in 1895.[7] Bronx County was separated from New York County (modern-day Manhattan) in 1914.[8] About a quarter of the Bronx's area is open space,[9] including Woodlawn Cemetery, Van Cortlandt Park, Pelham Bay Park, the New York Botanical Garden, and the Bronx Zoo in the borough's north and center. The Thain Family Forest at the New York Botanical Garden is thousands of years old and is New York City's largest remaining tract of the original forest that once covered the city.[10] These open spaces are primarily on land reserved in the late 19th century as urban development progressed north and east from Manhattan.\n",
    "\n",
    "The word \"Bronx\" originated with Swedish-born (or Faroese-born) Jonas Bronck, who established the first European settlement in the area as part of the New Netherland colony in 1639.[11][12][13] European settlers displaced the native Lenape after 1643. In the 19th and 20th centuries, the Bronx received many immigrant and migrant groups as it was transformed into an urban community, first from European countries particularly Ireland, Germany, Italy, and Eastern Europe, and later from the Caribbean region (particularly Puerto Rico, Trinidad, Haiti, Guyana, Jamaica, Barbados, and the Dominican Republic), and immigrants from West Africa (particularly from Ghana and Nigeria), African American migrants from the Southern United States, Panamanians, Hondurans, and South Asians.[14]\n",
    "\n",
    "The Bronx contains the poorest congressional district in the United States, New York's 15th. There are, however, some upper-income, as well as middle-income neighborhoods such as Riverdale, Fieldston, Spuyten Duyvil, Schuylerville, Pelham Bay, Pelham Gardens, Morris Park, and Country Club.[15][16][17] Parts of the Bronx saw a steep decline in population, livable housing, and quality of life starting from the mid-to-late 1960s, continuing throughout the 1970s and into the 1980s, ultimately culminating in a wave of arson in the late 1970s, a period when hip hop music evolved.[18] The South Bronx, in particular, experienced severe urban decay. The borough began experiencing new population growth starting in the late 1990s and continuing to the present day.[19]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534404f-8fb2-415b-ae0c-e311fd81b09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e166b4-4499-424a-bdea-8b190f669637",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = kw_model.extract_keywords(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a834edd9-514a-4f56-896d-9e8d40be4f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bronx', 0.6409),\n",
       " ('borough', 0.5789),\n",
       " ('westchester', 0.4554),\n",
       " ('brooklyn', 0.4442),\n",
       " ('york', 0.4363)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3ea2252-ee1a-4738-b1ce-c0aaf91e41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain the articles titles only for analysis\n",
    "titles_list = df.title.tolist()\n",
    "\n",
    "# Process the documents and collect the results\n",
    "titles_kws = kw_model.extract_keywords(titles_list, keyphrase_ngram_range=(1, 3))\n",
    "\n",
    "# Add the results to df\n",
    "df[\"titles_keywords\"] = titles_kws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d379bfeb-8dd4-4007-8f7e-c78aec77872d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [(policing 2024 debt, 0.7905), (stuff policing...\n",
       "1        [(inflation surge end, 0.731), (great inflatio...\n",
       "2        [(barn dodge viper, 0.8066), (dodge viper nigh...\n",
       "3        [(review ford bronco, 0.7184), (ford bronco sp...\n",
       "4        [(flesh eating bacteria, 0.6753), (eating bact...\n",
       "                               ...                        \n",
       "24098    [(obama wins creative, 0.6589), (creative arts...\n",
       "24099    [(2023 care court, 0.6707), (court bankrolling...\n",
       "24100    [(electric vehicle tax, 0.7099), (vehicle tax ...\n",
       "24101    [(starbucks wants 55, 0.5929), (stores 2030 ex...\n",
       "24102    [(2024 election guide, 0.8422), (election guid...\n",
       "Name: titles_keywords, Length: 24103, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"titles_keywords\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45dc8243-dada-47ec-b35c-f45960a962a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kws = df.titles_keywords.tolist()\n",
    "\n",
    "flat_keys = [item[0] for sublist in df_kws for item in sublist]\n",
    "\n",
    "flat_keys = list(set(flat_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8607b76b-d86c-474b-8b02-130d9c797cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_df = pd.DataFrame(flat_keys, columns = ['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "317e72d0-70a4-4f81-ae75-b2ef74c21081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ada9c8b-ed52-4640-a574-ed7c5ba96272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d9a2d20-04c0-4cad-97d4-c672234b3dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db976916-87cc-42e4-8824-9fcfedfc4468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb945e0d-edc2-4f8f-a945-d515aaa0a823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08db0c21e6824641a4e4c19726eb7082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate the embedding model\n",
    "# TODO: Figure out how to use CUDA\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device='cuda')\n",
    "\n",
    "# Embed the keywords and keyphrases into 768-dim real vector space\n",
    "# keys_df['key_bert'] = keys_df['key'].progress_apply(lambda x: model.encode(x))\n",
    "\n",
    "## This is way faster than row-based applies\n",
    "keys_df['key_bert']  = list(model.encode(keys_df['key'].tolist(), convert_to_tensor=False, show_progress_bar=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bfb1874-5409-4c7a-998a-db43e2f6ecf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(angular_rp_forest=True, metric='cosine', n_components=15, n_neighbors=25, verbose=True)\n",
      "Sun Jan 14 21:41:47 2024 Construct fuzzy simplicial set\n",
      "Sun Jan 14 21:41:47 2024 Finding Nearest Neighbors\n",
      "Sun Jan 14 21:41:47 2024 Building RP forest with 18 trees\n",
      "Sun Jan 14 21:41:54 2024 NN descent for 16 iterations\n",
      "\t 1  /  16\n",
      "\t 2  /  16\n",
      "\t 3  /  16\n",
      "\t 4  /  16\n",
      "\t 5  /  16\n",
      "\tStopping threshold met -- exiting after 5 iterations\n",
      "Sun Jan 14 21:42:14 2024 Finished Nearest Neighbor Search\n",
      "Sun Jan 14 21:42:17 2024 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06362b8d2d354723946c473f77af7c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/200 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  200 epochs\n",
      "\tcompleted  20  /  200 epochs\n",
      "\tcompleted  40  /  200 epochs\n",
      "\tcompleted  60  /  200 epochs\n",
      "\tcompleted  80  /  200 epochs\n",
      "\tcompleted  100  /  200 epochs\n",
      "\tcompleted  120  /  200 epochs\n",
      "\tcompleted  140  /  200 epochs\n",
      "\tcompleted  160  /  200 epochs\n",
      "\tcompleted  180  /  200 epochs\n",
      "Sun Jan 14 21:42:48 2024 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "# Reduce to 10-dimensional vectors and keep the local neighborhood at 15\n",
    "embeddings = umap.UMAP(n_neighbors=25, # Balances local vs. global structure.\n",
    "                       n_components=15, # Dimension of reduced vectors\n",
    "                       metric='cosine', verbose=True).fit_transform(list(keys_df.key_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f2da269-0bb7-4f94-8628-7001c2b16f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the reduced embedding vectors to the dataframe\n",
    "keys_df['key_umap'] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eedd15f4-26dc-48fb-a7b9-b05fb82bf855",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = int(len(keys_df)/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28e6df0d-c108-434d-a2f1-2e947b1db7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aec0eae8-3208-4f49-b000-5539fd86546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "837b3c11-8359-43c7-a52f-b16db67783bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Ran in 0.9396579265594482 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Initialize the clustering model\n",
    "clusterer = hdbscan.HDBSCAN(algorithm='best',\n",
    "                            prediction_data=True,\n",
    "                            approx_min_span_tree=True,\n",
    "                            gen_min_span_tree=True,\n",
    "                            min_cluster_size=min_df,\n",
    "                            cluster_selection_epsilon = .1,\n",
    "                            min_samples=1,\n",
    "                            p=None,\n",
    "                            metric='euclidean',\n",
    "                            cluster_selection_method='leaf')\n",
    "\n",
    "# Fit the data\n",
    "clusterer.fit(embeddings)\n",
    "\n",
    "# Create soft clusters\n",
    "soft_clusters = hdbscan.all_points_membership_vectors(clusterer)\n",
    "\n",
    "# Add the soft cluster information to the data\n",
    "closest_clusters = [np.argmax(x) for x in soft_clusters]\n",
    "keys_df['cluster'] = closest_clusters\n",
    "\n",
    "print(f\">> Ran in {(time.time()-start_time)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e304ac60-55c6-4b3f-bfaf-5d0c27569493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(closest_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebd2c92b-a7df-4846-9233-4b2358ed66bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>jennifer lopez ben</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>jennifer lawrence happier</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>natalie portman goes</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>cate blanchett</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>christina aguilera shows</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68685</th>\n",
       "      <td>meghan markle suits</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68769</th>\n",
       "      <td>amanda abbington claims</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68791</th>\n",
       "      <td>megyn kelly</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68822</th>\n",
       "      <td>worst dressed selena</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68832</th>\n",
       "      <td>2024 selena</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1080 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             key  cluster\n",
       "35            jennifer lopez ben      110\n",
       "40     jennifer lawrence happier      110\n",
       "78          natalie portman goes      110\n",
       "153               cate blanchett      110\n",
       "158     christina aguilera shows      110\n",
       "...                          ...      ...\n",
       "68685        meghan markle suits      110\n",
       "68769    amanda abbington claims      110\n",
       "68791                megyn kelly      110\n",
       "68822       worst dressed selena      110\n",
       "68832                2024 selena      110\n",
       "\n",
       "[1080 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.choice(closest_clusters)\n",
    "keys_df[keys_df['cluster']==i][['key', 'cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd5339-36cd-476a-9ce4-8c1c446a4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    model=llm,\n",
    "    tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    max_new_tokens=50,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "def extract_description(df: pd.DataFrame,\n",
    "                        n: int     \n",
    "                        )-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Use a custom prompt to send to a LLM\n",
    "    to extract labels and descriptions for a list of keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    one_cluster = df[df['cluster']==n]\n",
    "    one_cluster_copy = one_cluster.copy()\n",
    "    sample = one_cluster_copy.key.tolist()\n",
    "\n",
    "    prompt_clusters= f\"\"\"\n",
    "    <|system|>\n",
    "    I have the following list of keywords and keyphrases:\n",
    "    ['encryption','attribute','firewall','security properties',\n",
    "    'network security','reliability','surveillance','distributed risk factors',\n",
    "    'still vulnerable','cryptographic','protocol','signaling','safe',\n",
    "    'adversary','message passing','input-determined guards','secure communication',\n",
    "    'vulnerabilities','value-at-risk','anti-spam','intellectual property rights',\n",
    "    'countermeasures','security implications','privacy','protection',\n",
    "    'mitigation strategies','vulnerability','secure networks','guards']\n",
    "\n",
    "    Based on the information above, first name the domain these keywords or keyphrases \n",
    "  belong to, secondly give a brief description of the domain.\n",
    "    Do not use more than 30 words for the description!\n",
    "    Do not provide details!\n",
    "    Do not give examples of the contexts, do not say 'such as' and do not list the keywords \n",
    "  or the keyphrases!\n",
    "    Do not start with a statement of the form 'These keywords belong to the domain of' or \n",
    "  with 'The domain'.\n",
    "\n",
    "    Cybersecurity: Cybersecurity, emphasizing methods and strategies for safeguarding digital information\n",
    "    and networks against unauthorized access and threats.\n",
    "    </s>\n",
    "\n",
    "    <|user|>\n",
    "    I have the following list of keywords and keyphrases:\n",
    "    {sample}\n",
    "    Based on the information above, first name the domain these keywords or keyphrases belong to, secondly\n",
    "    give a brief description of the domain.\n",
    "    Do not use more than 30 words for the description!\n",
    "    Do not provide details!\n",
    "    Do not give examples of the contexts, do not say 'such as' and do not list the keywords or the keyphrases!\n",
    "    Do not start with a statement of the form 'These keywords belong to the domain of' or with 'The domain'.\n",
    "    <|assistant|>\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the outputs\n",
    "    outputs = generator(prompt_clusters,\n",
    "                    max_new_tokens=120,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.1,\n",
    "                    top_k=10,\n",
    "                    top_p=0.95)\n",
    "\n",
    "    text = outputs[0][\"generated_text\"]\n",
    "\n",
    "    # Example string\n",
    "    pattern = \"<|assistant|>\\n\"\n",
    "\n",
    "    # Extract the output\n",
    "    response = text.split(pattern, 1)[1].strip(\" \")\n",
    "    # Check if the output has the desired format\n",
    "    if len(response.split(\":\", 1)) == 2:\n",
    "        label  = response.split(\":\", 1)[0].strip(\" \")\n",
    "        description = response.split(\":\", 1)[1].strip(\" \")\n",
    "    else:\n",
    "        label = description = response\n",
    "\n",
    "    # Add the description and the labels to the dataframe\n",
    "    one_cluster_copy.loc[:, 'description'] = description\n",
    "    one_cluster_copy.loc[:, 'label'] = label\n",
    "\n",
    "    return one_cluster_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06cef958-adf7-43cf-b298-d979591ed220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c741a-249e-4268-b694-e444ed31b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/120 [00:00<?, ?it/s]C:\\Users\\Dan\\anaconda3\\Whisper\\lib\\site-packages\\transformers\\generation\\utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "  2%|█▉                                                                             | 3/120 [07:41<4:56:54, 152.26s/it]"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the cluster dataframes\n",
    "dataframes = []\n",
    "clusters = len(set(keys_df.cluster))\n",
    "\n",
    "# Iterate over the range of n values\n",
    "for n in tqdm(range(clusters-1)):\n",
    "    df_result = extract_description(keys_df,n)\n",
    "    dataframes.append(df_result)\n",
    "\n",
    "# Concatenate the individual dataframes\n",
    "final_df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a068aae-eda7-4eb9-a323-121621e7d2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
